{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juniverse/opt/anaconda3/envs/0u0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 그리고 인테리어에 가장 많이 사용되는 도배재료는 무엇인가요\n",
      "######### JH tokenizer ON  ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서: 인테리어에 사용되는 도배재료 중에서 어떤 소재가 가장 인기가 많은가요?\n",
      "점수: 2.181625198981345\n",
      "\n",
      "문서: 불연재료는 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 모던한 인테리어를 구성하는 데 가장 중요한 디자인 요소는 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 불량 도배지를 사용할 경우 도배지가 얼마나 오랫동안 사용 가능할까요?\n",
      "점수: 0.6751207995062278\n",
      "\n",
      "문서: 난연재료는 어떤 역할을 하는 거죠?\n",
      "점수: 0.765504929826097\n",
      "\n",
      "문서: 준불연재료에는 어떤 종류가 있나요?\n",
      "점수: 0.9318432427094396\n",
      "\n",
      "문서: 습도로 인해 도배지에 생기는 얼룩을 제거하는 데에 가장 효과적인 방법은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 인테리어에서 벽면에 사용되는 도장재료를 선택할 때 어떤 요소들을 고려해야 할까요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 어떤 종류의 인테리어 조명이 있죠?\n",
      "점수: 0.8838308378526607\n",
      "\n",
      "문서: 강화마루가 다른 바닥재료와 비교했을 때 어떤 장점이 있나요?\n",
      "점수: 0.8405235975287533\n",
      "\n",
      "########  BM25 tokenizer ON  #######\n",
      "\n",
      "문서: 인테리어에 사용되는 도배재료 중에서 어떤 소재가 가장 인기가 많은가요?\n",
      "점수: 5.197938612549444\n",
      "\n",
      "문서: 아스팔트슁글은 무엇을 위해 사용되는 재료인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 인테리어에 사용되는 조명의 종류에는 어떤 것들이 있을까요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 인테리어에서 벽면에 사용되는 도장재료를 선택할 때 어떤 요소들을 고려해야 할까요?\n",
      "점수: 0.7025046685908106\n",
      "\n",
      "문서: 벽지가 변색되는 가장 흔한 원인은 무엇인가요?\n",
      "점수: 0.8063856018276802\n",
      "\n",
      "문서: 도배할 수 없는 벽 대신에 어떤 재료를 사용할 수 있을까요?\n",
      "점수: 1.0362300848874746\n",
      "\n",
      "문서: 인테리어에서 사용되는 바닥재 중 목재 바닥은 어떤 장점들이 있는가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 친환경적인 인테리어를 위해 사용할 수 있는 친환경 재료는 어떤 것들이 있을까요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 도배를 하는데 가장 이상적인 계절은 어느 때인가요?\n",
      "점수: 0.8063856018276802\n",
      "\n",
      "문서: 모노벽돌은 주로 건축 및 벽돌 시공에서 사용되는 재료로, 어떤 특징이 있는지 알려주세요.\n",
      "점수: 0.8707667102660684\n"
     ]
    }
   ],
   "source": [
    "from retriever import JH_retriever\n",
    "if __name__ =='__main__':\n",
    "    import pandas as pd\n",
    "    train = pd.read_csv('open/train.csv')\n",
    "    test = pd.read_csv('open/test.csv')\n",
    "    \n",
    "    documents = list(train.질문_2) \n",
    "    querys    = list(test.질문)\n",
    "    \n",
    "    i = 39\n",
    "    query = querys[i].split('?')[1]\n",
    "    print(query)\n",
    "\n",
    "    top_n_documents,doc_scores = JH_retriever(documents,query,n=10,tokenizer_type='jh')\n",
    "    \n",
    "    for doc, score in zip(top_n_documents, doc_scores):\n",
    "        print(f\"문서: {doc}\")\n",
    "        print(f\"점수: {score}\")\n",
    "        print()\n",
    "        break\n",
    "        \n",
    "    top_n_documents,doc_scores = JH_retriever(documents,query,n=10,tokenizer_type='bm25')\n",
    "    \n",
    "    for doc, score in zip(top_n_documents, doc_scores):\n",
    "        print()\n",
    "        print(f\"문서: {doc}\")\n",
    "        print(f\"점수: {score}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['방청 페인트의 종류에는 어떤 것들이 있는지 알고 계신가요', ' 또한, 원목사이딩을 사용하는 것에 어떤 단점이 있을까요']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서: 방청 페인트는 어떤 기능을 가지고 있나요?\n",
      "점수: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서: 원목사이딩을 사용하는데 어떤 단점이 있을까요?\n",
      "점수: 1.4488224366383662\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "seperator = ['?','그리고','.','또한',',']\n",
    "query = spliter(querys[i],seperator)\n",
    "print(query)\n",
    "if query==str:\n",
    "    \n",
    "    top_n_documents,doc_scores = JH_retriever(documents,query,n=3,tokenizer_type='jh')\n",
    "\n",
    "    for doc, score in zip(top_n_documents, doc_scores):\n",
    "        print()\n",
    "        print(f\"문서: {doc}\")\n",
    "        print(f\"점수: {score}\")\n",
    "        break\n",
    "else:\n",
    "    for q in query:\n",
    "        top_n_documents,doc_scores = JH_retriever(documents,q,n=3,tokenizer_type='jh')\n",
    "    \n",
    "        for doc, score in zip(top_n_documents, doc_scores):\n",
    "            print()\n",
    "            print(f\"문서: {doc}\")\n",
    "            print(f\"점수: {score}\")\n",
    "            break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juniverse/opt/anaconda3/envs/0u0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from retriever import JH_retriever\n",
    "import pandas as pd\n",
    "train = pd.read_csv('open/train.csv')\n",
    "test = pd.read_csv('open/test.csv')\n",
    "\n",
    "documents = list(train.질문_2) \n",
    "querys    = list(test.질문)\n",
    "\n",
    "def spliter(query,seperator):\n",
    "    q = query\n",
    "    maxn = len(seperator)-1\n",
    "    for n,sep in enumerate(seperator):\n",
    "        if type(q)==str:\n",
    "            try:\n",
    "                query_split = q.split(sep=sep)\n",
    "                for i,q in enumerate(query_split):\n",
    "                    if len(q)<2:\n",
    "                        # del query_split[q]\n",
    "                        query_split.remove(q)\n",
    "                if len(query_split)>1:\n",
    "                    return query_split\n",
    "                else:\n",
    "                    q = query_split[0]\n",
    "                    if n ==maxn:\n",
    "                        return q\n",
    "            except:\n",
    "                print('nveve')\n",
    "        else:\n",
    "            print('no')\n",
    "            return q\n",
    "            \n",
    "\n",
    "seperator = ['?','그리고','.','또한',',']\n",
    "# for i in range():\n",
    "#     query = spliter(querys[i],seperator)\n",
    "#     print(query)\n",
    "#     if query==str:\n",
    "        \n",
    "#         top_n_documents,doc_scores = JH_retriever(documents,query,n=3,tokenizer_type='jh')\n",
    "    \n",
    "#         for doc, score in zip(top_n_documents, doc_scores):\n",
    "#             print()\n",
    "#             print(f\"문서: {doc}\")\n",
    "#             print(f\"점수: {score}\")\n",
    "#             break\n",
    "#     else:\n",
    "#         for q in query:\n",
    "#             top_n_documents,doc_scores = JH_retriever(documents,q,n=3,tokenizer_type='jh')\n",
    "        \n",
    "#             for doc, score in zip(top_n_documents, doc_scores):\n",
    "#                 print()\n",
    "#                 print(f\"문서: {doc}\")\n",
    "#                 print(f\"점수: {score}\")\n",
    "#                 break\n",
    "    \n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훼손과 오염은 어떻게 다른가요? \n",
      "########  BM25 tokenizer ON  #######\n",
      "\n",
      "문서: 훼손과 오염의 차이점은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 훼손은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 화학 물질로 인해 도배지가 오염될 수 있는 위험이 있나요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 도배지에 먼지 및 연기로 인한 오염이 발생할 경우 어떻게 처리해야 하나요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 벽장 부위 결로가 발생하는 주된 원인은 무엇일까요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 발코니 부위의 결로를 처리하는 가장 효과적인 방법은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 벽장 부위의 결로를 해결하는 방법에는 무엇이 있을까요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 지하 공간 결로를 예방하기 위해 어떤 조치를 취해야 할까요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 천장 부위 결로가 발생하는 가장 일반적인 원인은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 현관문 주위에 결로가 생기는 원인은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 천장 부위 결로를 방지하기 위한 가장 효과적인 방법은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: AD, PD에 면한 벽체 결로에 대한 대책은 어떤 것이 있나요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 부엌과 욕실에서 발생하는 결로의 주요 원인은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 지하 공간에서의 결로 대책으로 가장 효과적인 방법은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 발코니에 결로가 발생하는 주된 이유는 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 결로가 발생하는 기준은 무엇이며, 어떤 조건에서 결로로 인정되는지 알고 계신가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 부엌이나 욕실에서의 결로 방지를 위한 가장 효과적인 방법은 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 현관문 주위 결로를 막는 방법이 있을까요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 결로가 발생하는 주된 이유가 무엇인가요?\n",
      "점수: 0.0\n",
      "\n",
      "문서: 외벽 모서리 부위 결로를 해결하기 위한 가장 효과적인 방법은 무엇인가요?\n",
      "점수: 0.0\n",
      "훼손과 오염은 어떻게 다른가요? \n",
      "\n",
      "벽장 부위 결로가 발생하는 주된 원인은 무엇일까요?\n",
      "AD, PD에 면한 벽체 결로에 대한 대책은 어떤 것이 있나요?\n",
      "현관문 주위에 결로가 생기는 원인은 무엇인가요?\n",
      "발코니에 결로가 발생하는 주된 이유는 무엇인가요?\n",
      "부엌과 욕실에서 발생하는 결로의 주요 원인은 무엇인가요?\n",
      "천장 부위 결로가 발생하는 가장 일반적인 원인은 무엇인가요?\n",
      "도배지에 먼지 및 연기로 인한 오염이 발생할 경우 어떻게 처리해야 하나요?\n",
      "결로가 발생하는 기준은 무엇이며, 어떤 조건에서 결로로 인정되는지 알고 계신가요?\n",
      "지하 공간에서의 결로 대책으로 가장 효과적인 방법은 무엇인가요?\n",
      "훼손은 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "from retriever import JH_retriever\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('open/train.csv')\n",
    "test = pd.read_csv('open/test.csv')\n",
    "\n",
    "documents = list(train.질문_2) \n",
    "querys    = list(test.질문)\n",
    "\n",
    "i = 1\n",
    "query = querys[i].split('또한, ')[0]\n",
    "print(query)\n",
    "\n",
    "# top_n_documents_jh,doc_scores = JH_retriever(documents,query,n=10,tokenizer_type='jh')\n",
    "\n",
    "# for doc, score in zip(top_n_documents, doc_scores):\n",
    "#     print(f\"문서: {doc}\")\n",
    "#     print(f\"점수: {score}\")\n",
    "#     print()\n",
    "    \n",
    "top_n_documents_bm25,doc_scores = JH_retriever(documents,query,n=20,tokenizer_type='bm25')\n",
    "\n",
    "for doc, score in zip(top_n_documents_bm25, doc_scores):\n",
    "    print()\n",
    "    print(f\"문서: {doc}\")\n",
    "    print(f\"점수: {score}\")\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "documents1  = list(train.질문_1)\n",
    "documents2  = list(train.질문_2)\n",
    "querys = list(test.질문)\n",
    "# qeurys = querys[:4]\n",
    "model_path = 'model00ver3.pt'\n",
    "model = torch.load(model_path,map_location=torch.device('cpu'))\n",
    "\n",
    "model_ckpt = 'sentence-transformers/distiluse-base-multilingual-cased-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt) # 사용하는 토크나이저\n",
    "# model = SentenceTransformer('distiluse-base-multilingual-cased-v1') # 이 모델을 사용할거임.\n",
    "\n",
    "x = tokenizer.batch_encode_plus([query],return_tensors='pt',padding=True,truncation=True,max_length=30)\n",
    "y1 = tokenizer.batch_encode_plus(top_n_documents_bm25,return_tensors='pt',padding=True,truncation=True,max_length=30)\n",
    "y2 = tokenizer.batch_encode_plus(top_n_documents_bm25,return_tensors='pt',padding=True,truncation=True,max_length=30)\n",
    "\n",
    "q = model(x).sentence_embedding\n",
    "a1 = model(y1).sentence_embedding\n",
    "a2 = model(y2).sentence_embedding\n",
    "\n",
    "W1 = q@a1.T\n",
    "sorted, indices = torch.sort(W1)\n",
    "print(query)\n",
    "print()\n",
    "\n",
    "sorts = list(indices[0].numpy())\n",
    "for j in range(10):\n",
    "    print(top_n_documents_bm25[sorts.index(len(top_n_documents_bm25)-j-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부실 시공으로 인해 타공하자가 발생할 가능성이 있나요?\n",
      "\n",
      "훼손과 오염은 어떻게 다른가요? 또한, 부실 시공으로 인해 타공하자가 발생할 가능성이 있나요?\n",
      "면진장치가 뭐야?\n",
      "\n",
      "건조시간이 부족하면 도배지에 울음이 발생할 수 있다는데, 왜 건조시간이 부족하면 울음이 발생하는 건가요?\n",
      "외단열 시공에 비해 내단열 시공의 단점이 어떻게 되나요?\n",
      "마감재의 하자를 어떻게 확인할 수 있을까요?\n",
      "부실 시공으로 인해 타공하자가 발생하는 경우가 있나요?\n",
      "높은 습도로 인해 몰딩수정이 발생하는데 대처할 방법이 있을까요?\n",
      "내단열 시공을 하는 것의 장점은 무엇인가요?\n",
      "구조적 결함으로 인해 석고수정이 발생할 가능성이 있는가요?\n",
      "높은 습도로 인해 도배할 때 패턴이 이어지지 않을 가능성이 있나요?\n",
      "외단열 시공이 내단열 시공보다 안좋은 점이 뭐야?\n",
      "벽지 안쪽의 못 등 작은 피스로 인해 발생하는 하자를 어떻게 해결해야 하죠?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "documents1  = list(train.질문_1)\n",
    "documents2  = list(train.질문_2)\n",
    "querys = list(test.질문)\n",
    "# qeurys = querys[:4]\n",
    "model = torch.load('model00ver3.pt',map_location=torch.device('cpu'))\n",
    "\n",
    "model_ckpt = 'sentence-transformers/distiluse-base-multilingual-cased-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt) # 사용하는 토크나이저\n",
    "# model = SentenceTransformer('distiluse-base-multilingual-cased-v1') # 이 모델을 사용할거임.\n",
    "\n",
    "x = tokenizer.batch_encode_plus([query],return_tensors='pt',padding=True,truncation=True,max_length=30)\n",
    "y1 = tokenizer.batch_encode_plus(top_n_documents_bm25,return_tensors='pt',padding=True,truncation=True,max_length=30)\n",
    "y2 = tokenizer.batch_encode_plus(top_n_documents_bm25,return_tensors='pt',padding=True,truncation=True,max_length=30)\n",
    "\n",
    "q = model(x).sentence_embedding\n",
    "a1 = model(y1).sentence_embedding\n",
    "a2 = model(y2).sentence_embedding\n",
    "\n",
    "W1 = q@a1.T\n",
    "sorted, indices = torch.sort(W1)\n",
    "print(query)\n",
    "print()\n",
    "\n",
    "argmax = torch.argmax(W1)\n",
    "print(querys[i])\n",
    "print(documents1[argmax])\n",
    "print()\n",
    "\n",
    "\n",
    "sorts = list(indices[0].numpy())\n",
    "for j in range(10):\n",
    "    print(top_n_documents_bm25[sorts.index(len(top_n_documents_bm25)-j-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9556, 0.9236, 0.9563, 0.9395, 0.9773, 0.8525, 0.8957, 0.8697, 0.9388,\n",
      "         0.7703]], grad_fn=<DivBackward0>)\n",
      "tensor([[4, 2, 0, 3, 8, 1, 6, 7, 5, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "def retrieve(query, documents, k=10):\n",
    "    \"\"\"\n",
    "    KoBERT 기반 Retriever 함수\n",
    "\n",
    "    Args:\n",
    "        query (str): 질문 문장\n",
    "        documents (list): 문서 리스트\n",
    "        k (int): 상위 k개 문서 추출\n",
    "\n",
    "    Returns:\n",
    "        list: 상위 k개 문서 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    # Query 토크나이징\n",
    "    query_inputs = tokenizer.batch_encode_plus([query])\n",
    "    # Query 벡터 계산\n",
    "    # query_embedding = model(**query_inputs).pooler_output\n",
    "    query_embedding = model(input_ids = torch.tensor(query_inputs['input_ids']),attention_mask = torch.tensor(query_inputs['attention_mask'])).pooler_output\n",
    "    \n",
    "    # 문서 벡터 계산\n",
    "    document_embeddings = torch.tensor([])\n",
    "    for document in documents:\n",
    "        document_inputs = tokenizer.batch_encode_plus([document])\n",
    "        document_embedding = model(input_ids = torch.tensor(document_inputs['input_ids']),attention_mask = torch.tensor(document_inputs['attention_mask'])).pooler_output\n",
    "        # document_embedding = model(**document_inputs).pooler_output\n",
    "        document_embeddings =torch.concat((document_embeddings,document_embedding),dim=0)\n",
    "\n",
    "    # Cosine 유사도 계산\n",
    "    similarities = torch.matmul(query_embedding, document_embeddings.T) / torch.norm(query_embedding) / torch.norm(document_embeddings, dim=1)\n",
    "    print(similarities)\n",
    "    # 상위 k개 문서 추출\n",
    "    top_k_indices = torch.argsort(similarities, descending=True)[:k]\n",
    "    print(top_k_indices)\n",
    "    # top_k_documents = [documents[i] for i in top_k_indices.tolist()]\n",
    "    top_k_documents = [documents[i] for i in top_k_indices.view(-1).tolist()]\n",
    "    return top_k_documents,document_embeddings\n",
    "top_k_documents,document_embeddings = retrieve(query,top_n_documents_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../open/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../open/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../open/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m documents1  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train\u001b[38;5;241m.\u001b[39m질문_1)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/0u0/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/0u0/lib/python3.10/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/0u0/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/0u0/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/0u0/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../open/train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "train = pd.read_csv('../open/train.csv')\n",
    "test = pd.read_csv('../open/test.csv')\n",
    "\n",
    "documents1  = list(train.질문_1)\n",
    "documents2  = list(train.질문_2)\n",
    "querys = list(test.질문)\n",
    "# qeurys = querys[:4]\n",
    "model = torch.load('model00ver3.pt',map_location=torch.device('cpu'))\n",
    "\n",
    "model_ckpt = 'sentence-transformers/distiluse-base-multilingual-cased-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt) # 사용하는 토크나이저\n",
    "# model = SentenceTransformer('distiluse-base-multilingual-cased-v1') # 이 모델을 사용할거임.\n",
    "\n",
    "x = tokenizer.batch_encode_plus(querys,return_tensors='pt',padding=True,truncation=True,max_length=30)\n",
    "y1 = tokenizer.batch_encode_plus(documents1,return_tensors='pt',padding=True,truncation=True,max_length=30)\n",
    "y2 = tokenizer.batch_encode_plus(documents2,return_tensors='pt',padding=True,truncation=True,max_length=30)\n",
    "\n",
    "q = model(x).sentence_embedding\n",
    "a1 = model(y1).sentence_embedding\n",
    "a2 = model(y2).sentence_embedding\n",
    "# for n,p in model.named_parameters():\n",
    "#     print(n,p.shape)\n",
    "\n",
    "W = q@a1.T\n",
    "\n",
    "####### 제일 높은값 한개만 나타내기.\n",
    "for i in range(130):\n",
    "    argmax = torch.argmax(W[i])\n",
    "    print(querys[i])\n",
    "    print(documents1[argmax])\n",
    "\n",
    "    if i==3:\n",
    "        break\n",
    "    \n",
    "\n",
    "sorted, indices = torch.sort(W)\n",
    "print(indices.shape)\n",
    "\n",
    "print('------------ ------------ ------------ ------------ ------------')\n",
    "\n",
    "####### 순서대로 여러개 나타내기\n",
    "for i in range(130):\n",
    "    print(querys[i])\n",
    "    sorts = list(indices[i].numpy())\n",
    "    for j in range(5):\n",
    "        print(documents1[sorts.index(643-j)])\n",
    "    print()\n",
    "    if i==3:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0u0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
