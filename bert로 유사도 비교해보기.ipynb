{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/dacon-llm/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/dacon-llm/test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 유사도 구하기 : 평균 값 : 0.79386\n",
    "# 전체 샘플의 Cosine Similarity Score 평균 :  0.79386\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertConfig\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1') # 539M\n",
    "\n",
    "# 문장 예시\n",
    "preds = [\n",
    "    \"이번 경진대회는 질의 응답 처리를 수행하는 AI 모델을 개발해야합니다.\",\n",
    "    \"데이콘은 플랫폼입니다.\"\n",
    "]\n",
    "\n",
    "gts = [\n",
    "    \"이번 경진대회의 주제는 도배 하자 질의 응답 AI 모델 개발입니다.\",\n",
    "    \"데이콘은 국내 최대의 AI 경진대회 플랫폼입니다.\"\n",
    "]\n",
    "\n",
    "preds = list(train.질문_1)\n",
    "gts  = list(train.질문_2)\n",
    "\n",
    "# 샘플에 대한 Cosine Similarity 산식\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b) if norm_a != 0 and norm_b != 0 else 0\n",
    "\n",
    "sample_scores = []\n",
    "for pred, gt in zip(preds, gts):\n",
    "    # 생성된 답변 내용을 512 Embedding Vector로 변환\n",
    "    pred_embed = model.encode(pred)\n",
    "    gt_embed = model.encode(gt)\n",
    "    \n",
    "    print(gt_embed.shape)\n",
    "    sample_score = cosine_similarity(gt_embed, pred_embed)\n",
    "    # Cosine Similarity Score가 0보다 작으면 0으로 간주\n",
    "    sample_score = max(sample_score, 0)\n",
    "    print('예측 : ', pred)\n",
    "    print('정답 : ', gt)\n",
    "    print('Cosine Similarity Score : ', sample_score)\n",
    "    print('-'*20)\n",
    "    \n",
    "    sample_scores.append(sample_score)\n",
    "    \n",
    "print(sample_scores)\n",
    "print('전체 샘플의 Cosine Similarity Score 평균 : ', np.mean(sample_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(sample_scores,bins=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skt/kobert_v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 유사도 구하기 : base-encoder 값 : 0.7939\n",
    "# skt/kobert  : 0.6921 - 0인 값들을 빼도 0.74 많이 부족함..\n",
    "\n",
    "# 전체 샘플의 Cosine Similarity Score 평균 :  0.79386\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertConfig\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1') # 539M\n",
    "\n",
    "\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "tokenizer.encode(\"한국어 모델을 공유합니다.\")\n",
    "#[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "\n",
    "preds = list(train.질문_1)\n",
    "gts  = list(train.질문_2)\n",
    "\n",
    "# 샘플에 대한 Cosine Similarity 산식\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b) if norm_a != 0 and norm_b != 0 else 0\n",
    "\n",
    "sample_scores = []\n",
    "for pred, gt in zip(preds, gts):\n",
    "    # 생성된 답변 내용을 512 Embedding Vector로 변환\n",
    "    \n",
    "    inputs = tokenizer.batch_encode_plus([pred])\n",
    "    out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
    "              attention_mask = torch.tensor(inputs['attention_mask']))\n",
    "    \n",
    "    pred_embed = out.pooler_output.detach().numpy().reshape(-1)\n",
    "    \n",
    "    \n",
    "    inputs = tokenizer.batch_encode_plus([gt])\n",
    "    out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
    "              attention_mask = torch.tensor(inputs['attention_mask']))\n",
    "    gt_embed =  out.pooler_output.detach().numpy().reshape(-1)\n",
    "    \n",
    "    sample_score = cosine_similarity(gt_embed, pred_embed)\n",
    "    # Cosine Similarity Score가 0보다 작으면 0으로 간주\n",
    "    sample_score = max(sample_score, 0)\n",
    "    print('예측 : ', pred)\n",
    "    print('정답 : ', gt)\n",
    "    print('Cosine Similarity Score : ', sample_score)\n",
    "    print('-'*20)\n",
    "    \n",
    "    sample_scores.append(sample_score)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sample_scores)\n",
    "print('전체 샘플의 Cosine Similarity Score 평균 : ', np.mean(sample_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sample_scores,bins=80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
